diagram of this project:
https://prnt.sc/HUIljZAWzggT
just me an open source(vagrant niye video)

#try to install kubernets using vagrant
#virtual box

apt update -y
apt install net-tools -y
apt install tcpdump -y
apt install vim -y

# Install ping command for network testing
apt install iputils-ping -y

# Create resources from the service1-rc.yaml file
kubectl create -f service1-rc.yaml

# Create resources from the service-svc.yaml file
kubectl create -f service-svc.yaml

# List all pods in the current namespace
kubectl get pods

# List all services in the default namespace
kubectl get svc

# Get details of a specific service named server1-service
kubectl get svc server1-service

# Get endpoint information for services
kubectl get ep

# List all pods with additional details like IP and node info
kubectl get pods -o wide

# Get node IP and other details (note the typo: "nnode" should be "nodes")
kubectl get nodes -o wide

# Delete a specific pod by its name
kubectl delete pod <pod name>

# Build a Docker image and tag it for Docker Hub
docker build -t <dockerhub account id name>/<image name> .

# Push the Docker image to Docker Hub
docker push <dockerhub account id name>/<image name>

# Open an interactive shell in a running pod
kubectl exec -it loadbalancer-rc-9d272 /bin/bash

# Get all services in the default namespace (run on your local machine)
kubectl get svc -n default

# Test service connectivity via HTTP using curl
curl http://server1-service:80

# Forward a local port to a port on a Kubernetes service for testing
kubectl port-forward svc/server1-service 8080:80

# Set a specific nodePort for external access to a service
nodePort: 30002 

# Get the YAML configuration for a specific service
kubectl get svc server1-service -o yaml

# Describe a specific service to see detailed information
kubectl describe svc <service-name>

# Capture and analyze network traffic on port 80 with tcpdump
tcpdump -i any port 80 -vv

# Check logs for a specific pod
kubectl logs <pod name>

# View resource usage (CPU/Memory) in a namespace
kubectl top pod -n <namespace>

# Get logs from a specific container in a pod
kubectl logs <pod-name> -c <container-name>

# Describe a node to get detailed information (useful for debugging)
kubectl describe node <node-name>

# Run a command in a new pod (useful for testing/debugging)
kubectl run -i --tty --rm debug --image=busybox -- sh

# Delete a pod by name (same as earlier, but worth repeating)
kubectl delete pod <pod-name>

# Delete all pods in a namespace
kubectl delete pods --all -n <namespace>

# Check the status of all pods, even those in a failed or completed state
kubectl get pods --all-namespaces -o wide

# Create a deployment from a YAML file
kubectl apply -f deployment.yaml

# Scale a deployment to a specific number of replicas
kubectl scale deployment <deployment-name> --replicas=<number-of-replicas>

# Get detailed information about a deployment
kubectl describe deployment <deployment-name>

# List all namespaces in the cluster
kubectl get namespaces

# Create a new namespace
kubectl create namespace <namespace-name>

# Switch to a different namespace
kubectl config set-context --current --namespace=<namespace-name>





#Why this set up ?
#Clarification of Workflow:
1. Pod Communication:

Pods communicate with each other or with external resources via Services. A Pod doesn't need to know the IP 
addresses of other Pods directly; it interacts with them through a Service, which abstracts away the complexity.

2. Service Communication:

When a request is directed at a Service, Kubernetes routes the request to one of the Pods that match the Service’s label selector. 
This selection process includes built-in load balancing, where requests are distributed among all available Pods.

3. External Load Balancing:

If your application needs to handle external traffic, a load balancer (such as Nginx in your Docker setup) can be configured to route 
incoming traffic to various Kubernetes Services. These Services, in turn, route traffic to the appropriate Pods. 
This setup ensures that external users can reach your application while maintaining a balanced load distribution across your application’s 
instances.

Example Workflow:
1. External User Request: An external user sends a request to your application’s URL.
2. Nginx Load Balancer: The request first hits the Nginx load balancer, which is configured to route requests to one of the Kubernetes Services.
3. Service Routing: The Service then forwards the request to one of the Pods it manages, ensuring that the traffic is balanced across 
multiple instances of the application.
4. Pod Handling: The selected Pod processes the request and returns the response, which is then routed back through the Service, 
load balancer, and finally to the external user.